links

Background

The map / reduce paradigm has been discussed in various forms since XXX [REFS], and describes an approach to parallel
processing. Fundamentally, the paradigm consists of two steps. In the *map* phase, a task is broken down into small chunks, each of which may be processed on different nodes of a cluster (or different cores on multi-core CPU). The output of each of the tasks is then aggregated in the *reduce* phase. A more formal definition is given below.

Given that map / reduce can be considered a design pattern [REF], there are many implementations of this in a variety of languages. For example, the map phase is easily carried out in languages such as Python (the *map* command), R (the *apply* or *lapply* commands) and Lisp (the *map* command). Thus while many languages support the map/reduce paradigm, the real value is the ability to use this approach to process massive datasets over large compute clusters. This scenario ws described in by Ghemawat and has spurred the development of a number of map/reduce *frameworks*, which allow one to take advantage of large clusters. Probably the most well known such framework is Hadoop, first developed by Yahoo Inc. and subsequently open sourced. The framework allows one to process arbitrarily large datasets, over very large clusters. For example, Yahoo has reported the analysis of multi-terabyte web log files on clusters with 4000 nodes [REF]. 

Many applications of the map/reduce approach have focused on *trivially parallelizable* problems. In these cases, the computational processing step is relatively simple, but must be applied a large number of objects (which may be individual files or even individual lines in a single, very large file). Examples of this include log file analysis [REF], image process [REF] and document analysis [REF]. On the other hand there are a number of cases, where traditional serial algorithms can be converted in to a map/reduce framework In such cases, the dataset itself may not be particularly large, but the computational processing can be sped up by being distributed over a cluster. Examples of such appplications include linguistic analysis [REF], outlier detection [REF]. In addition, there have been reports of a variety of machine learning algorithms being converted to the map/reduce framework including Bayesian networks [REF], discriminant classification [REF] and XXX.

While there has been active development of map/reduce fragments in a variety of fields, such developments for life science applications have not been as widespread. Most of the current scientific applications have focused on bioinformatics methodologies. A recent example is the case of Bowtie [REF], which is an implementation of genome assembly algorithms in the map/reduce framework. 

In contrast, there have been reports of cheminformatics applications making use of the map/reduce pardigm, either at the dataset or algorithmic levels. One of the major reasons for this is that most datasets in cheminformatics are not very large (at least compared to those generated in other fields such as genome sequencing). On an algorithmic level, certain cheminformatics methods (such as thosed based on machine learning) can make use of map/reduce frameworks. Whether it is useful to do so brings us back to the issue of dataset size.

Outline

The rest of this paper describes the use of the Hadoop map/reduce framework for cheminformatics problems. Specifically we focus on the use of Hadoop version 0.18. While more recent versions are available, this is the version that is support by Amazon, thus allowing us to test our applications on the Amazon cloud, without modifications to the source code. Section XXX describes how one designs a Hadoop program to perform such computations. We specifically highlight input/output issues that arise when one moves from line oriented formats (i.e., SMILES files) to multi-line record formats (such as SD files). We then describes examples of map/reduce applications. We consider data-parallel problems as well as algorithm-parallel applciations. Section XXX provides some benchmarks comparing Hadoop programs to their serial counterparts. We also provide some initial benchmarks for programs running on Amazon Elastic Compute servers. Finally, Section XXX discusses some of the drawbacks of the map/reduce paradigm and use of Amazon services as well as further applications in cheminformatics.

Methods

Hadoop is a general purpose Java framework to support map/reduce style programs. While Hadoop programs can be run on a single machine with a single CPU, it's primary goal is to support such applications on large clusters. Thus, one can implement a Hadoop program and test it on ones local machine and then deploy it as is to a large cluster for production runs. It should also be noted that Hadoop programs do not necessarily have to be Java programs. It is posible to use programs written in arbitrary languages with Hadoop, as long they can read from standard input and write to standard output. In addition to the Hadoop framework, we require the use of a cheminformatics library to actually perform the domain tasks. We employ the Chemistry Development Kit (CDK), an Open Source Java cheminformatics toolkit [REFS].

Data Flow in Hadoop Programs

To improve I/O efficiency, most Hadoop programs prefer to work with a few large files, rather a large number of small files. We assume that a single large file is to be processed. When a Hadoop program is started, it will *chunk* the file, sending each chunk to an individual node. For text files, a chunk is defined in terms of bytes - by default, each chunk is 64MB, though this can be changed at run time. In general, if the processing time for an individual record is small, larger chunks will be preferred.  Note that since chunks are defined by size, there is no guarantee that a chunk begins at the start of a record or ends at the end of  record. In the case of data files where each line is a record, Hadoop will automatically provide complete lines to the mapper classes. For multi-line records, the user must provide an appropriate reader. Once a chunk has been created and sent to a node, it is then processed by the mapper class, which will receive one record at a time. Each node will process the records in its chunk and emit a new set of records in the form <key, value>. Once the first set of records are available from the mappers, the reducer classes will aggregate the key, value pairs such that all pairs with the same key are collected and then processed. In simple Hadoop programs, the output of the reducer classes is the output of the program. However, there is no restriction on the number of map and reduce phases, so that the output from the first reduce phase could be fed back into a second map phase. It is also possible to avoid the reduce phase in some cases (see Section XXX). While always having a reduce phase does not affect the final results, being able to avoid a reduce phase can lead to more efficient Hadoop programs.

 
Mappers and Reducers

The key to developing a Hadoop program is to implement classes for the map and reduce phases.
A schematic outline of these two classes are shown in Listings XXX and YYY. When the mapper class is instantiated and called, it receives a single record as a String object.

Listing XXX

    public static class BasicMapper extends Mapper<Object, Text, Text, IntWritable> {

        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {

            // value is the record
            // key is an arbitrary indentifier for this record
            // we write the result of the map task to context

        }
    }


Listing YYY

    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            
            // key is that generated by the mapper
            // values are the values corresponding to the key
            // we compute a result using these values and output to context

        }
    }

Input/Output

By default, Hadoop allows one to write map/reduce programs that deal with single-line oriented data files. That is each line represents a single record. In cheminformatics, SMILES files are an example of such records. Thus if the input is a SMILES file, one need not worry about file processing and can simply implement the map and reduce classes.

However many applications require the use of multi-line records such as SD files, CML files and so on. To support such files, so that the mappers are able to access individual records, one must implement custom readers. We have provided a a class to support the reading of SD files in a Hadoop program.

Applications

We first consider the cheminformatics equivalent of the canonical word counting Hadoop application. In the original example, the goal is to calculate the frequency of words in a very large text file. In the cheminformatics version, we are interested in counting the frequency of individual elements in a collection of SMILES strings. The program is provided in Listing ZZZ. The key features are the mapper and reducer classes. In the former, the *map* method receives a single line from the SMILES file, along with an arbitrary key. The line is parsed as SMILES string giving us an object representing the molecules.  We then loop over the atoms in the molecule and for each atom we write out the symbol along with a constant integer (viz., 1). This represents the output of the mapper - a series of key, value pairs, in which the key is the atomic symbol and its value simply indicates that the symbol occurred. One can see that by summing up the values for a given key, one would obtain the number of times that the element occurs in the collection of SMILES strings. Note that the output of the mapper is written to a special object of class Context (as opposed to STDOUT). This allows the Hadoop framwork to aggregate the key,value output of the mappers (which are likely running on different nodes) which can then be provided to the reducer class.

At one point, after sufficient results from the mappers have been obtained, the framework will instantiate the reducer class. In this example, the reducer class will receive a single key and the values associated with that key. For the atom counting case, a key could be the symbol "C" and the values would be the series {1,1,1,1,1}. In this application the reduce operation need only sum up the values and as with the mapper, writes out the key and it's reduced value to the Context object. Hadoop can then collect the output of the reducers (which will in general be running on multiple nodes) and generate the final output of the program.

it is important to note that the output type of the mapper should match the input type of the reducer. In this example, the output of the mapper is a series of String,Integer pairs (the variable "one" is an instance of IntWritable). As a result, the reducer will expect that the values corresponding to a given key to be a series of Integers. Thus the second argument of the reduce method in IntSumReducer is Iterable<IntWritable>. This is also indicated in the signature of the mapper and reducer classes. 

Benchmarks

To run the applications described here on ones local machine, one must set up a Hadoop File System (HFS). Into this filesystem, one loads the input data files and on completion of the run, one retrieves the output files. For Hadoop version 0.18 one can initialize the HFS as follows:

foo
bar

With the filesystem initialized, one copies a data file from the local disk into the HFS by:

foo
bar

At this point we can run the program. We compile the program into a single jar file, contaning all the dependencies. With this jar file (all programs described here can be obtained in JAR form from XXX) one then runs the program as

foo
bar

Progress can usually be monitored via a web browser at XXX, allowing one to view the progress of the mappers and reducers. Obviously, on a single machine this is not particularly informative, but is useful for debugging purposes. One benefit of using the Hadoop provided configuration classes is that one can switch to serial execution from the command line. In this mode, the input file is still chunked, but each chunk is processed serially. This allows us to easily generate a number of benchmarks.

Discussion

* Why not use a job scheduling system - these address the map phase, but would require extra work to perform the reduce phase.
* The nature of the file *chunking* step suggests the use of very large input files. For applications based on SMILES input, it is rare to have multi-gigabyte input files (the whole of PubChem in SMILES format is XXX GB). In such cases, one can reduce the default chunk size, allowing the use of multiple mapper nodes. However, the amount of data in a given chunk becomes smaller and the overhead of the data distrbution step may overshadow the actual computation time. This issue becomes unimportant when dealing with large SD files.
* map reduce is bad - http://developers.slashdot.org/story/08/01/18/1813248/MapReduce-mdash-a-Major-Step-Backwards
